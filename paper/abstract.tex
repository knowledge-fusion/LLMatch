Schema matching is one of the fundamenal building blocks of data integration.
The task involves identifying the correspondences between the attributes of two schemas.
The problem is challenging due to the heterogeneity of the data sources and the lack of a common vocabulary.
Added to the complexity is the fact that in most multi-table relational databases, the correspondences are not one-to-one.
Due to different schema normalization techniques, the same information may be stored in different tables and columns.
Unfortunately, many machine-learning-based solutions cannot capture the complex foreign key relationships between the tables.
As a result, most of the existing schema matching tools and benchmarks focus on the one-table-to-one-table matching problem.
With the breakthroughs in large language models (LLM), we can now leverage the power of these models to capture the complex relationships between the tables.
In this paper, we study the problem of schema matching in multi-table relational databases using LLMs as the underlying tool.
How to manage context size and how to expose the right amount of context to achieve optimal performance.
We also construct a new benchmark for schema matching in multi-table relational databases.
We evaluate our approach on both existing and new benchmarks and show that it outperforms the state-of-the-art schema matching tools.
We identify that the performance of the LLMs is highly dependent on the quality of context exposed to the model and the quality of context does not correlate with the context size.
This observation provides a key insight to build LLM-based applications.

